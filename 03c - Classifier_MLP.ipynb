{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "import torch\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from IPython.display import clear_output\n",
    "from auxiliar_functions import (\n",
    "    save_data,\n",
    "    process_folds,\n",
    "    build_report,\n",
    "    load_dataset,\n",
    "    predict_deep,\n",
    "    load_embedding,\n",
    ")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish') + [\"UNK\"]\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>encoded</th>\n",
       "      <th>frames</th>\n",
       "      <th>conflicto</th>\n",
       "      <th>economico</th>\n",
       "      <th>humanidad</th>\n",
       "      <th>moral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>[8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UDI acusa \"mala memoria\" de la Nueva Mayoría f...</td>\n",
       "      <td>udi acusa mala memoria de la nueva mayoría fre...</td>\n",
       "      <td>[9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...</td>\n",
       "      <td>[1, 0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La misteriosa oferta por Esteban Paredes que i...</td>\n",
       "      <td>la misteriosa oferta por esteban paredes que [...</td>\n",
       "      <td>[9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La familia maratón que causó revuelo en Holand...</td>\n",
       "      <td>la familia maratón que causó revuelo en holand...</td>\n",
       "      <td>[9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crean sitio web que recopila mangas descontin...</td>\n",
       "      <td>crean sitio web que [UNK] [UNK] [UNK] para [UN...</td>\n",
       "      <td>[2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  Japón registró un nuevo déficit comercial réco...   \n",
       "1  UDI acusa \"mala memoria\" de la Nueva Mayoría f...   \n",
       "2  La misteriosa oferta por Esteban Paredes que i...   \n",
       "3  La familia maratón que causó revuelo en Holand...   \n",
       "4   Crean sitio web que recopila mangas descontin...   \n",
       "\n",
       "                                     preprocess_text  \\\n",
       "0  japón registró un nuevo déficit comercial réco...   \n",
       "1  udi acusa mala memoria de la nueva mayoría fre...   \n",
       "2  la misteriosa oferta por esteban paredes que [...   \n",
       "3  la familia maratón que causó revuelo en holand...   \n",
       "4  crean sitio web que [UNK] [UNK] [UNK] para [UN...   \n",
       "\n",
       "                                             encoded        frames conflicto  \\\n",
       "0  [8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...  [0, 1, 0, 0]         0   \n",
       "1  [9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...  [1, 0, 0, 1]         1   \n",
       "2  [9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...  [1, 0, 0, 0]         1   \n",
       "3  [9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...  [0, 0, 1, 0]         0   \n",
       "4  [2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...  [0, 1, 0, 0]         0   \n",
       "\n",
       "  economico humanidad moral  \n",
       "0         1         0     0  \n",
       "1         0         0     1  \n",
       "2         0         0     0  \n",
       "3         0         1     0  \n",
       "4         1         0     0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset(\"preprocess_dataset.npy\")\n",
    "Y_true = np.array([np.array(x) for x in df.frames])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(torch.nn.Module):\n",
    "    def __init__(self, layer_input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(layer_input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = self.relu(output)\n",
    "        return self.dropout(output)\n",
    "    \n",
    "    \n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[50, 150], n_epochs=30, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        layer_input_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(Layer(layer_input_size, hidden_size))\n",
    "            layer_input_size = hidden_size\n",
    "            \n",
    "        self.final_fc = torch.nn.Linear(layer_input_size, 4)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        return self.final_fc(output)\n",
    "    \n",
    "    def get_proba(self, x):\n",
    "        return self.sigmoid(self(x))    \n",
    "        \n",
    "    def fit(self, X, Y, X_val=None, Y_val=None, writer=None, fold_index=None):\n",
    "        model = self.cuda(1)\n",
    "        \n",
    "        ds_train = TensorDataset(torch.Tensor(X), torch.Tensor(Y))\n",
    "        train_dl = DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "        dataloaders = {\"train\": train_dl}\n",
    "        \n",
    "        if X_val is not None:\n",
    "            ds_test = TensorDataset(torch.Tensor(X_val), torch.Tensor(Y_val))    \n",
    "            dataloaders[\"val\"] = DataLoader(ds_test, batch_size=32)\n",
    "        \n",
    "        loss_object = torch.nn.BCEWithLogitsLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=0.001)  \n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            for step in dataloaders:\n",
    "                if step == \"train\":\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    \n",
    "                total_data = 0.0\n",
    "                total_corrects = 0.0\n",
    "                total_loss = 0\n",
    "                loss_class = [0, 0, 0, 0]\n",
    "                all_logits = None\n",
    "                all_target = None\n",
    "\n",
    "                for x, target in dataloaders[step]:\n",
    "                    optimizer.zero_grad()                    \n",
    "                    \n",
    "                    x = x.cuda(1)\n",
    "                    target = target.cuda(1)                   \n",
    "                    logits = model(x)          \n",
    "\n",
    "                    loss = loss_object(logits, target)\n",
    "                    total_loss += loss * (target.shape[0] * 4)\n",
    "\n",
    "                    for i in range(4):\n",
    "                        loss_c = loss_object(logits[:, i], target[:, i])\n",
    "                        loss_class[i] += loss_c * target.shape[0]\n",
    "\n",
    "                    preds = logits >= 0.0\n",
    "                    if all_logits is None:\n",
    "                        all_logits = preds.detach().cpu().numpy()\n",
    "                        all_target = target.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        all_logits = np.append(all_logits, preds.detach().cpu().numpy(), axis=0)\n",
    "                        all_target = np.append(all_target, target.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "                    total_data += (target.shape[0]*4)      \n",
    "                    \n",
    "                    if step == \"train\":\n",
    "                        loss.backward()                             \n",
    "                        optimizer.step()                            \n",
    "                        correctas = (preds == target).sum().item() \n",
    "                        total_corrects += correctas               \n",
    "                        accuracy = total_corrects/total_data \n",
    "                        \n",
    "                        print(\"\\rEpoch {}: Loss: {:.4f} Accuracy: {:.2f}%\".format(epoch, loss, 100*accuracy),\n",
    "                              end=\"\")\n",
    "\n",
    "                save_data(writer, all_logits, all_target, total_loss, loss_class,\n",
    "                          total_data, fold_index, epoch, step)\n",
    "                 \n",
    "                    \n",
    "    def predict_proba(self, X):\n",
    "        model = self.cuda(1)\n",
    "        model.eval()\n",
    "        \n",
    "        ds_test = TensorDataset(torch.Tensor(X))\n",
    "        test_dl = DataLoader(ds_test, batch_size=1)\n",
    "        all_probs = None\n",
    "        for x, in test_dl:\n",
    "            x = x.cuda(1)                \n",
    "            probs = self.get_proba(x)\n",
    "\n",
    "            if all_probs is None:\n",
    "                all_probs = probs.detach().cpu().numpy()\n",
    "            else:\n",
    "                all_probs = np.append(all_probs, probs.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        return all_probs \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(mode, model_name, args):\n",
    "    print(f\"{'#'*50}\\n\\n{mode}\\n\\n{'#'*50}\")\n",
    "    \n",
    "    if \"tf-idf\" not in mode:\n",
    "        embedding_train = load_embedding(mode, \"dataset\")\n",
    "    else: # mode == \"tf-idf\":\n",
    "        embedding_train = df.preprocess_text.values\n",
    "    \n",
    "    writer = None #SummaryWriter(f\"runs/{model_name}_{mode}\")\n",
    "    \n",
    "    np.random.seed(4444)\n",
    "    torch.manual_seed(4444)\n",
    "    \n",
    "    datasets = []\n",
    "    k_fold = IterativeStratification(n_splits=10, order=1)\n",
    "\n",
    "    for fold_index, (train, test) in enumerate(k_fold.split(embedding_train, Y_true)):\n",
    "        print(f'\\nFolds {fold_index + 1}')\n",
    "        \n",
    "        Y_train_fold, Y_test_fold = Y_true[train], Y_true[test]\n",
    "        X_train_fold, X_test_fold = embedding_train[train], embedding_train[test]\n",
    "        \n",
    "        if \"tf-idf\" in mode:\n",
    "            tf_idf_pipeline = Pipeline([\n",
    "                  ('vect', CountVectorizer(lowercase=False, stop_words = spanish_stopwords)),\n",
    "                  ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "                  ])\n",
    "\n",
    "            tf_idf_pipeline.fit(X_train_fold)\n",
    "            X_train_fold = tf_idf_pipeline.transform(X_train_fold).toarray()\n",
    "            X_test_fold = tf_idf_pipeline.transform(X_test_fold).toarray()\n",
    "\n",
    "        args[\"input_size\"] = X_train_fold.shape[1]\n",
    "        \n",
    "        model = MLP(**args)\n",
    "        model.fit(X_train_fold, Y_train_fold, X_test_fold, Y_test_fold, writer, fold_index)\n",
    "        torch.save(model.state_dict(), f'Models/Fold_{fold_index+1}_{model_name}_{mode}.model')\n",
    "        \n",
    "        frames_probability = model.predict_proba(X_test_fold).tolist()\n",
    "        y_pred = [[int(a >= 0.5) for a in x] for x in frames_probability]\n",
    "\n",
    "        df_r = pd.DataFrame()\n",
    "        df_r[\"y_pred\"] = y_pred\n",
    "        df_r[\"y_prob\"] = frames_probability\n",
    "        df_r[\"y_true\"] = Y_test_fold.tolist()\n",
    "        \n",
    "        data = process_folds([df_r])\n",
    "        datasets.append(df_r.copy())\n",
    "    \n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "    data = process_folds(datasets)\n",
    "    build_report(pd.DataFrame(data.mean()).T, pd.DataFrame(data.std()).T, model_name)\n",
    "\n",
    "    with open(f\"Results/cross_validation#{model_name}#{mode}.pickle\", \"wb\") as file:\n",
    "        pickle.dump(datasets, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "\n",
      "tf-idf\n",
      "\n",
      "##################################################\n",
      "\n",
      "Folds 1\n",
      "Epoch 69: Loss: 0.2261 Accuracy: 96.40%\n",
      "Folds 2\n",
      "Epoch 69: Loss: 0.1678 Accuracy: 96.39%\n",
      "Folds 3\n",
      "Epoch 69: Loss: 0.1981 Accuracy: 96.52%\n",
      "Folds 4\n",
      "Epoch 69: Loss: 0.1972 Accuracy: 96.46%\n",
      "Folds 5\n",
      "Epoch 69: Loss: 0.2190 Accuracy: 96.42%\n",
      "Folds 6\n",
      "Epoch 69: Loss: 0.1675 Accuracy: 96.50%\n",
      "Folds 7\n",
      "Epoch 69: Loss: 0.1918 Accuracy: 96.38%\n",
      "Folds 8\n",
      "Epoch 69: Loss: 0.1699 Accuracy: 96.38%\n",
      "Folds 9\n",
      "Epoch 69: Loss: 0.2160 Accuracy: 96.39%\n",
      "Folds 10\n",
      "Epoch 69: Loss: 0.1701 Accuracy: 96.50%\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.79(±0.02)       0.80(±0.02)       0.79(±0.02)\n",
      "               Macro       0.78(±0.03)       0.69(±0.03)       0.72(±0.03)       0.69(±0.04)       0.83(±0.03)\n",
      "        \n",
      "##################################################\n",
      "\n",
      "fasttext\n",
      "\n",
      "##################################################\n",
      "\n",
      "Folds 1\n",
      "Epoch 69: Loss: 0.4747 Accuracy: 80.04%\n",
      "Folds 2\n",
      "Epoch 69: Loss: 0.4182 Accuracy: 80.11%\n",
      "Folds 3\n",
      "Epoch 69: Loss: 0.3690 Accuracy: 79.94%\n",
      "Folds 4\n",
      "Epoch 69: Loss: 0.4952 Accuracy: 79.99%\n",
      "Folds 5\n",
      "Epoch 69: Loss: 0.4819 Accuracy: 79.93%\n",
      "Folds 6\n",
      "Epoch 69: Loss: 0.4754 Accuracy: 80.05%\n",
      "Folds 7\n",
      "Epoch 69: Loss: 0.4482 Accuracy: 79.83%\n",
      "Folds 8\n",
      "Epoch 69: Loss: 0.4158 Accuracy: 79.82%\n",
      "Folds 9\n",
      "Epoch 69: Loss: 0.3865 Accuracy: 79.97%\n",
      "Folds 10\n",
      "Epoch 69: Loss: 0.3716 Accuracy: 79.94%\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.78(±0.02)       0.79(±0.02)       0.78(±0.02)\n",
      "               Macro       0.76(±0.03)       0.68(±0.03)       0.70(±0.03)       0.66(±0.04)       0.83(±0.02)\n",
      "        \n",
      "##################################################\n",
      "\n",
      "elmo\n",
      "\n",
      "##################################################\n",
      "\n",
      "Folds 1\n",
      "Epoch 69: Loss: 0.2816 Accuracy: 79.82%\n",
      "Folds 2\n",
      "Epoch 69: Loss: 0.3947 Accuracy: 79.83%\n",
      "Folds 3\n",
      "Epoch 69: Loss: 0.4563 Accuracy: 79.87%\n",
      "Folds 4\n",
      "Epoch 69: Loss: 0.4254 Accuracy: 79.85%\n",
      "Folds 5\n",
      "Epoch 69: Loss: 0.3062 Accuracy: 79.87%\n",
      "Folds 6\n",
      "Epoch 69: Loss: 0.3525 Accuracy: 79.90%\n",
      "Folds 7\n",
      "Epoch 69: Loss: 0.5409 Accuracy: 79.71%\n",
      "Folds 8\n",
      "Epoch 69: Loss: 0.3895 Accuracy: 79.79%\n",
      "Folds 9\n",
      "Epoch 69: Loss: 0.4536 Accuracy: 79.87%\n",
      "Folds 10\n",
      "Epoch 69: Loss: 0.3963 Accuracy: 80.05%\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.77(±0.02)       0.78(±0.02)       0.76(±0.02)\n",
      "               Macro       0.73(±0.03)       0.67(±0.04)       0.68(±0.04)       0.64(±0.05)       0.81(±0.02)\n",
      "        \n",
      "##################################################\n",
      "\n",
      "beto_embedding_mean\n",
      "\n",
      "##################################################\n",
      "\n",
      "Folds 1\n",
      "Epoch 69: Loss: 0.5146 Accuracy: 83.43%\n",
      "Folds 2\n",
      "Epoch 69: Loss: 0.4101 Accuracy: 83.34%\n",
      "Folds 3\n",
      "Epoch 69: Loss: 0.4374 Accuracy: 83.13%\n",
      "Folds 4\n",
      "Epoch 69: Loss: 0.3606 Accuracy: 83.39%\n",
      "Folds 5\n",
      "Epoch 69: Loss: 0.3925 Accuracy: 83.40%\n",
      "Folds 6\n",
      "Epoch 69: Loss: 0.3592 Accuracy: 83.43%\n",
      "Folds 7\n",
      "Epoch 69: Loss: 0.3866 Accuracy: 83.28%\n",
      "Folds 8\n",
      "Epoch 69: Loss: 0.3635 Accuracy: 83.02%\n",
      "Folds 9\n",
      "Epoch 69: Loss: 0.3654 Accuracy: 83.17%\n",
      "Folds 10\n",
      "Epoch 69: Loss: 0.3534 Accuracy: 83.35%\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.79(±0.02)       0.80(±0.02)       0.79(±0.02)\n",
      "               Macro       0.76(±0.03)       0.71(±0.03)       0.73(±0.03)       0.70(±0.04)       0.84(±0.02)\n",
      "        \n",
      "##################################################\n",
      "\n",
      "beto_embedding_cls\n",
      "\n",
      "##################################################\n",
      "\n",
      "Folds 1\n",
      "Epoch 69: Loss: 0.5353 Accuracy: 84.75%\n",
      "Folds 2\n",
      "Epoch 69: Loss: 0.4112 Accuracy: 84.32%\n",
      "Folds 3\n",
      "Epoch 69: Loss: 0.3687 Accuracy: 84.57%\n",
      "Folds 4\n",
      "Epoch 69: Loss: 0.3725 Accuracy: 84.60%\n",
      "Folds 5\n",
      "Epoch 69: Loss: 0.3951 Accuracy: 84.63%\n",
      "Folds 6\n",
      "Epoch 69: Loss: 0.3425 Accuracy: 84.53%\n",
      "Folds 7\n",
      "Epoch 69: Loss: 0.3792 Accuracy: 84.60%\n",
      "Folds 8\n",
      "Epoch 69: Loss: 0.3657 Accuracy: 84.64%\n",
      "Folds 9\n",
      "Epoch 69: Loss: 0.3335 Accuracy: 84.56%\n",
      "Folds 10\n",
      "Epoch 69: Loss: 0.3322 Accuracy: 84.61%\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.78(±0.02)       0.79(±0.02)       0.78(±0.02)\n",
      "               Macro       0.74(±0.03)       0.70(±0.03)       0.71(±0.03)       0.66(±0.05)       0.82(±0.02)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "for embedding in [\"tf-idf\", \"fasttext\", \"elmo\", \"beto_embedding_mean\", \"beto_embedding_cls\"]:\n",
    "    cross_validation(embedding, \"MLP-LR\",  {'hidden_sizes': [], 'n_epochs': 70})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "\n",
      "tf-idf\n",
      "\n",
      "##################################################\n",
      "\n",
      "Folds 1\n",
      "Epoch 4: Loss: 0.3688 Accuracy: 88.73%\n",
      "Folds 2\n",
      "Epoch 4: Loss: 0.2439 Accuracy: 89.26%\n",
      "Folds 3\n",
      "Epoch 4: Loss: 0.3040 Accuracy: 88.49%\n",
      "Folds 4\n",
      "Epoch 4: Loss: 0.3074 Accuracy: 88.91%\n",
      "Folds 5\n",
      "Epoch 4: Loss: 0.2632 Accuracy: 89.07%\n",
      "Folds 6\n",
      "Epoch 4: Loss: 0.2446 Accuracy: 88.62%\n",
      "Folds 7\n",
      "Epoch 4: Loss: 0.2019 Accuracy: 88.56%\n",
      "Folds 8\n",
      "Epoch 4: Loss: 0.3013 Accuracy: 88.61%\n",
      "Folds 9\n",
      "Epoch 4: Loss: 0.2361 Accuracy: 88.86%\n",
      "Folds 10\n",
      "Epoch 4: Loss: 0.3095 Accuracy: 88.38%\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.79(±0.02)       0.80(±0.02)       0.79(±0.02)\n",
      "               Macro       0.77(±0.03)       0.69(±0.03)       0.71(±0.03)       0.69(±0.04)       0.83(±0.02)\n",
      "        \n",
      "##################################################\n",
      "\n",
      "fasttext\n",
      "\n",
      "##################################################\n",
      "\n",
      "Folds 1\n",
      "Epoch 4: Loss: 0.4719 Accuracy: 76.97%\n",
      "Folds 2\n",
      "Epoch 4: Loss: 0.4750 Accuracy: 76.82%\n",
      "Folds 3\n",
      "Epoch 4: Loss: 0.3788 Accuracy: 77.04%\n",
      "Folds 4\n",
      "Epoch 4: Loss: 0.4407 Accuracy: 76.99%\n",
      "Folds 5\n",
      "Epoch 4: Loss: 0.4755 Accuracy: 76.74%\n",
      "Folds 6\n",
      "Epoch 4: Loss: 0.4787 Accuracy: 76.67%\n",
      "Folds 7\n",
      "Epoch 4: Loss: 0.5014 Accuracy: 77.09%\n",
      "Folds 8\n",
      "Epoch 4: Loss: 0.4710 Accuracy: 76.86%\n",
      "Folds 9\n",
      "Epoch 4: Loss: 0.4583 Accuracy: 76.86%\n",
      "Folds 10\n",
      "Epoch 4: Loss: 0.4901 Accuracy: 77.53%\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.76(±0.03)       0.77(±0.02)       0.75(±0.02)\n",
      "               Macro       0.74(±0.05)       0.64(±0.03)       0.65(±0.04)       0.63(±0.04)       0.80(±0.02)\n",
      "        \n",
      "##################################################\n",
      "\n",
      "elmo\n",
      "\n",
      "##################################################\n",
      "\n",
      "Folds 1\n",
      "Epoch 4: Loss: 0.3796 Accuracy: 75.69%\n",
      "Folds 2\n",
      "Epoch 4: Loss: 0.4567 Accuracy: 75.51%\n",
      "Folds 3\n",
      "Epoch 4: Loss: 0.5001 Accuracy: 75.16%\n",
      "Folds 4\n",
      "Epoch 4: Loss: 0.4489 Accuracy: 75.84%\n",
      "Folds 5\n",
      "Epoch 4: Loss: 0.5236 Accuracy: 76.07%\n",
      "Folds 6\n",
      "Epoch 4: Loss: 0.5179 Accuracy: 75.95%\n",
      "Folds 7\n",
      "Epoch 4: Loss: 0.5329 Accuracy: 75.77%\n",
      "Folds 8\n",
      "Epoch 4: Loss: 0.4719 Accuracy: 76.11%\n",
      "Folds 9\n",
      "Epoch 4: Loss: 0.5102 Accuracy: 75.56%\n",
      "Folds 10\n",
      "Epoch 4: Loss: 0.5047 Accuracy: 75.86%\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.75(±0.02)       0.76(±0.01)       0.74(±0.02)\n",
      "               Macro       0.73(±0.04)       0.62(±0.03)       0.63(±0.04)       0.59(±0.05)       0.78(±0.02)\n",
      "        \n",
      "##################################################\n",
      "\n",
      "beto_embedding_mean\n",
      "\n",
      "##################################################\n",
      "\n",
      "Folds 1\n",
      "Epoch 4: Loss: 0.4590 Accuracy: 79.22%\n",
      "Folds 2\n",
      "Epoch 4: Loss: 0.4944 Accuracy: 78.65%\n",
      "Folds 3\n",
      "Epoch 4: Loss: 0.4776 Accuracy: 78.66%\n",
      "Folds 4\n",
      "Epoch 4: Loss: 0.4569 Accuracy: 78.45%\n",
      "Folds 5\n",
      "Epoch 4: Loss: 0.3519 Accuracy: 78.80%\n",
      "Folds 6\n",
      "Epoch 4: Loss: 0.3945 Accuracy: 78.72%\n",
      "Folds 7\n",
      "Epoch 4: Loss: 0.4129 Accuracy: 78.45%\n",
      "Folds 8\n",
      "Epoch 4: Loss: 0.4763 Accuracy: 78.14%\n",
      "Folds 9\n",
      "Epoch 4: Loss: 0.5094 Accuracy: 78.68%\n",
      "Folds 10\n",
      "Epoch 4: Loss: 0.4767 Accuracy: 78.67%\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.78(±0.02)       0.79(±0.02)       0.77(±0.02)\n",
      "               Macro       0.76(±0.03)       0.68(±0.03)       0.69(±0.04)       0.67(±0.04)       0.83(±0.02)\n",
      "        \n",
      "##################################################\n",
      "\n",
      "beto_embedding_cls\n",
      "\n",
      "##################################################\n",
      "\n",
      "Folds 1\n",
      "Epoch 4: Loss: 0.5215 Accuracy: 79.95%\n",
      "Folds 2\n",
      "Epoch 4: Loss: 0.4709 Accuracy: 79.77%\n",
      "Folds 3\n",
      "Epoch 4: Loss: 0.4294 Accuracy: 79.29%\n",
      "Folds 4\n",
      "Epoch 4: Loss: 0.4464 Accuracy: 79.18%\n",
      "Folds 5\n",
      "Epoch 4: Loss: 0.3094 Accuracy: 80.10%\n",
      "Folds 6\n",
      "Epoch 4: Loss: 0.4038 Accuracy: 79.71%\n",
      "Folds 7\n",
      "Epoch 4: Loss: 0.3685 Accuracy: 79.68%\n",
      "Folds 8\n",
      "Epoch 4: Loss: 0.4545 Accuracy: 79.56%\n",
      "Folds 9\n",
      "Epoch 4: Loss: 0.4829 Accuracy: 79.64%\n",
      "Folds 10\n",
      "Epoch 4: Loss: 0.4513 Accuracy: 79.57%\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.79(±0.02)       0.79(±0.02)       0.78(±0.02)\n",
      "               Macro       0.76(±0.03)       0.69(±0.03)       0.70(±0.04)       0.67(±0.04)       0.83(±0.02)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "for embedding in [\"tf-idf\", \"fasttext\", \"elmo\", \"beto_embedding_mean\", \"beto_embedding_cls\"]:\n",
    "    cross_validation(embedding, \"MLP-1\",  {'hidden_sizes': [150], 'n_epochs': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
