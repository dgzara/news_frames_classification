{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from auxiliar_functions import process_folds, build_report, load_dataset, predict_deep, load_embedding\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import torch \n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from AsymmetricLoss import AsymmetricLossOptimized\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>encoded</th>\n",
       "      <th>frames</th>\n",
       "      <th>conflicto</th>\n",
       "      <th>economico</th>\n",
       "      <th>humanidad</th>\n",
       "      <th>moral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>[8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UDI acusa \"mala memoria\" de la Nueva Mayoría f...</td>\n",
       "      <td>udi acusa mala memoria de la nueva mayoría fre...</td>\n",
       "      <td>[9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...</td>\n",
       "      <td>[1, 0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La misteriosa oferta por Esteban Paredes que i...</td>\n",
       "      <td>la misteriosa oferta por esteban paredes que [...</td>\n",
       "      <td>[9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La familia maratón que causó revuelo en Holand...</td>\n",
       "      <td>la familia maratón que causó revuelo en holand...</td>\n",
       "      <td>[9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crean sitio web que recopila mangas descontin...</td>\n",
       "      <td>crean sitio web que [UNK] [UNK] [UNK] para [UN...</td>\n",
       "      <td>[2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  Japón registró un nuevo déficit comercial réco...   \n",
       "1  UDI acusa \"mala memoria\" de la Nueva Mayoría f...   \n",
       "2  La misteriosa oferta por Esteban Paredes que i...   \n",
       "3  La familia maratón que causó revuelo en Holand...   \n",
       "4   Crean sitio web que recopila mangas descontin...   \n",
       "\n",
       "                                     preprocess_text  \\\n",
       "0  japón registró un nuevo déficit comercial réco...   \n",
       "1  udi acusa mala memoria de la nueva mayoría fre...   \n",
       "2  la misteriosa oferta por esteban paredes que [...   \n",
       "3  la familia maratón que causó revuelo en holand...   \n",
       "4  crean sitio web que [UNK] [UNK] [UNK] para [UN...   \n",
       "\n",
       "                                             encoded        frames conflicto  \\\n",
       "0  [8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...  [0, 1, 0, 0]         0   \n",
       "1  [9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...  [1, 0, 0, 1]         1   \n",
       "2  [9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...  [1, 0, 0, 0]         1   \n",
       "3  [9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...  [0, 0, 1, 0]         0   \n",
       "4  [2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...  [0, 1, 0, 0]         0   \n",
       "\n",
       "  economico humanidad moral  \n",
       "0         1         0     0  \n",
       "1         0         0     1  \n",
       "2         0         0     0  \n",
       "3         0         1     0  \n",
       "4         1         0     0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset(\"preprocess_dataset.npy\")\n",
    "Y_true = np.array([np.array(x) for x in df.frames])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plantaciones', 'lamentando', 'cerrados', 'alessandro', 'consejero'] ['en', 'el', 'la', '[NUM]', 'de'] 10000\n"
     ]
    }
   ],
   "source": [
    "vocab = [x.strip().split(\": \")[0] for x in open(\"vocabulary_corpus_counter.txt\", encoding=\"UTF-8\").readlines()]\n",
    "\n",
    "print(vocab[:5], vocab[-5:], len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vocabulary\n",
    "vocab_to_idx = {\"[PAD]\":0, \"[UNK]\":1}\n",
    "WORDS = [\"[PAD]\", \"[UNK]\"]\n",
    "for word in vocab:\n",
    "    vocab_to_idx[word] = len(WORDS)\n",
    "    WORDS.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDER = ['conflicto', 'economico', 'humanidad', 'moral']\n",
    "\n",
    "class MyTextDataset(Dataset):\n",
    "    def __init__(self, X, Y=None):\n",
    "        self.texts = [torch.LongTensor(x) for x in X]\n",
    "        self.len = len(X)\n",
    "        if Y is not None:\n",
    "            self.frames = Y\n",
    "        else:\n",
    "            self.frames = [np.array([0]*4) for _ in range(self.len)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        x = self.texts[item]\n",
    "        y = self.frames[item]\n",
    "        return x, y\n",
    "    \n",
    "    \n",
    "def my_collate(data_list):\n",
    "    data_list.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    X_seq, Y = zip(*data_list)\n",
    "    lengths = [len(x) for x in X_seq]\n",
    "    X = torch.nn.utils.rnn.pad_sequence(X_seq, batch_first=True)\n",
    "    return ((X, lengths), torch.Tensor(Y))\n",
    "\n",
    "\n",
    "def calculate_pres_recall(preds, Y):\n",
    "    pres_class = [0] * Y.shape[1]\n",
    "    recall_class = [0] * Y.shape[1]\n",
    "    acc_class = [0] * Y.shape[1]\n",
    "\n",
    "    all_y_pred = []\n",
    "    all_y_true = []\n",
    "    for i in range(Y.shape[1]):\n",
    "        y_pred = [int(pred[i]) for pred in preds]\n",
    "        y_true = [int(target[i]) for target in Y]\n",
    "\n",
    "        all_y_pred.extend(y_pred)\n",
    "        all_y_true.extend(y_true)\n",
    "\n",
    "        pres_class[i] = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall_class[i] = recall_score(y_true, y_pred, zero_division=0)\n",
    "        acc_class[i] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    mean_pres = precision_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    mean_recall = recall_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    mean_acc = accuracy_score(all_y_true, all_y_pred)\n",
    "\n",
    "    return mean_pres, mean_recall, mean_acc, pres_class, recall_class, acc_class \n",
    "\n",
    "\n",
    "def save_data(writer, all_logits, Y, total_loss, loss_class,\n",
    "              total_muestras, fold_index, epoch, step):\n",
    "    if writer is None:\n",
    "        return\n",
    "\n",
    "    loss = (total_loss/total_muestras).item()\n",
    "    pres, recall, acc, pres_class, recall_class, acc_class = calculate_pres_recall(all_logits, Y)\n",
    "\n",
    "    writer.add_scalar(f'Fold_{fold_index}/loss_{step}', loss, epoch)\n",
    "    writer.add_scalar(f'Fold_{fold_index}/recall_{step}', recall, epoch)\n",
    "    writer.add_scalar(f'Fold_{fold_index}/presicion_{step}', pres, epoch)\n",
    "    writer.add_scalar(f'Fold_{fold_index}/acc_{step}', acc, epoch)\n",
    "\n",
    "    for i in range(len(loss_class)):\n",
    "        loss_class_train = loss_class[i]/(total_muestras/Y.shape[1])\n",
    "        writer.add_scalar(f'Fold_{fold_index}/loss_class_{ORDER[i]}_{step}', loss_class_train, epoch)\n",
    "        writer.add_scalar(f'Fold_{fold_index}/presicion_{ORDER[i]}_{step}', pres_class[i], epoch)\n",
    "        writer.add_scalar(f'Fold_{fold_index}/recall_{ORDER[i]}_{step}', recall_class[i], epoch)\n",
    "        writer.add_scalar(f'Fold_{fold_index}/acc_{ORDER[i]}_{step}', acc_class[i], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_dim_lstm=50, hidden_size=50,\n",
    "                 n_epochs=30, process_output=\"max\", batch_size=1, \n",
    "                 loss_function=\"cross-entropy\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_epochs = n_epochs\n",
    "        self.hidden_dim_lstm = hidden_dim_lstm\n",
    "        self.process_output = process_output\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_object = AsymmetricLossOptimized()\n",
    "        if loss_function == \"cross-entropy\":\n",
    "            self.loss_object = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    \n",
    "        self.embedding = torch.nn.Embedding(len(WORDS), embedding_size, padding_idx=0)\n",
    "        self.lstm = torch.nn.LSTM(embedding_size, hidden_dim_lstm, \n",
    "                                  num_layers=2, bidirectional=True,\n",
    "                                  batch_first=True)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(hidden_dim_lstm*2, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, 4)\n",
    "        self.sigmoid = torch.nn.Sigmoid()    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.get_embedding(x)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        x, lengths = x\n",
    "        output = self.embedding(x)\n",
    "        \n",
    "        output = torch.nn.utils.rnn.pack_padded_sequence(output, lengths, batch_first=True)\n",
    "        lstm_out, (hidden, _) = self.lstm(output) # shape = (batch_size, seq_len, hidden_dim*2)\n",
    "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True, padding_value=0.0)\n",
    "        output = lstm_out\n",
    "        \n",
    "        if self.process_output == \"max\":    \n",
    "            output, _ = torch.max(output, 1) # shape = (batch_size, hidden_dim*2)\n",
    "        elif self.process_output == \"mean\":\n",
    "            mask = output != 0.0\n",
    "            output = (output*mask).sum(dim=1)/mask.sum(dim=1)\n",
    "        else: # last_state       # layers # direction (2) # batch size, # hidden_dim\n",
    "            hidden = hidden.view(2, 2, len(lengths), self.hidden_dim_lstm)[-1]\n",
    "            output = torch.cat([hidden[0], hidden[1]], 1)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def get_proba(self, x):\n",
    "        return self.sigmoid(self(x))\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y, X_val=None, Y_val=None, writer=None, fold_index=None):\n",
    "        model = self.cuda()\n",
    "        \n",
    "        ds_train = MyTextDataset(X, Y)\n",
    "        train_dl = DataLoader(ds_train, batch_size=self.batch_size, shuffle=True, collate_fn=my_collate)\n",
    "        dataloaders = {\"train\": train_dl}\n",
    "        \n",
    "        if X_val is not None:\n",
    "            ds_val = MyTextDataset(X_val, Y_val)    \n",
    "            dataloaders[\"val\"] = DataLoader(ds_val, batch_size=self.batch_size, collate_fn=my_collate)\n",
    "        \n",
    "        loss_object = self.loss_object\n",
    "        optimizer = AdamW(model.parameters(), lr=0.001)  \n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            for step in dataloaders:\n",
    "                if step == \"train\":\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    \n",
    "                total_muestras = 0.0\n",
    "                total_correctas = 0.0\n",
    "                total_loss = 0\n",
    "                loss_class = [0, 0, 0, 0]\n",
    "                all_logits = None\n",
    "                all_target = None\n",
    "\n",
    "                for (x, length), target in dataloaders[step]:\n",
    "                    optimizer.zero_grad()                    \n",
    "                    \n",
    "                    x = x.cuda()\n",
    "                    target = target.cuda()                   \n",
    "                    logits = model((x, length))          \n",
    "\n",
    "                    loss = loss_object(logits, target)\n",
    "                    total_loss += loss * (target.shape[0] * target.shape[1])\n",
    "\n",
    "                    for i in range(target.shape[1]):\n",
    "                        loss_c = loss_object(logits[:, i], target[:, i])\n",
    "                        loss_class[i] += loss_c * target.shape[0]\n",
    "\n",
    "                    preds = logits >= 0.0\n",
    "                    if all_logits is None:\n",
    "                        all_logits = preds.detach().cpu().numpy()\n",
    "                        all_target = target.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        all_logits = np.append(all_logits, preds.detach().cpu().numpy(), axis=0)\n",
    "                        all_target = np.append(all_target, target.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "                    total_muestras += (target.shape[0]*target.shape[1])\n",
    "                    \n",
    "                    if step == \"train\":\n",
    "                        loss.backward()                             \n",
    "                        optimizer.step()                            \n",
    "                        correctas = (preds == target).sum().item() \n",
    "                        total_correctas += correctas               \n",
    "                        accuracy = total_correctas/total_muestras \n",
    "                        \n",
    "                        print(\"\\rEpoca {}: Loss: {:.4f} Accuracy: {:.2f}%\".format(epoch, loss, 100*accuracy),\n",
    "                              end=\"\")\n",
    "\n",
    "                save_data(writer, all_logits, all_target, total_loss, loss_class,\n",
    "                          total_muestras, fold_index, epoch, step)\n",
    "                 \n",
    "                    \n",
    "    def predict_proba(self, X):\n",
    "        model = self.cuda()\n",
    "        model.eval()\n",
    "        \n",
    "        ds_test = MyTextDataset(X)    \n",
    "        test_dl = DataLoader(ds_test, batch_size=1, collate_fn=my_collate)\n",
    "        all_probs = None\n",
    "        for (x, length), _ in test_dl:\n",
    "            x = x.cuda()                \n",
    "            probs = self.get_proba((x, length))\n",
    "\n",
    "            if all_probs is None:\n",
    "                all_probs = probs.detach().cpu().numpy()\n",
    "            else:\n",
    "                all_probs = np.append(all_probs, probs.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        return all_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(args, name):\n",
    "    writer = SummaryWriter('runs/' + name)\n",
    "    \n",
    "    embedding_train = np.array([np.array(x) for x in df.encoded.values], dtype=object)\n",
    "    \n",
    "    np.random.seed(4444)\n",
    "    torch.manual_seed(4444)\n",
    "\n",
    "    datasets = []\n",
    "    k_fold = IterativeStratification(n_splits=10, order=1)\n",
    "\n",
    "    for fold_index, (train, test) in enumerate(k_fold.split(embedding_train, Y_true)):\n",
    "\n",
    "        Y_train_fold, Y_test_fold = Y_true[train], Y_true[test]\n",
    "        X_train_fold, X_test_fold = embedding_train[train], embedding_train[test]\n",
    "\n",
    "        model = BiLSTM(**args)\n",
    "        model.fit(X_train_fold, Y_train_fold, X_test_fold, Y_test_fold, writer, fold_index)\n",
    "        torch.save(model.state_dict(), f'Models/Fold_{fold_index+1}_{name}.model')\n",
    "\n",
    "        frames_probability = model.predict_proba(X_test_fold).tolist()\n",
    "        y_pred = [[int(pred >= 0.5) for pred in frames] for frames in frames_probability]\n",
    "\n",
    "        df_result = pd.DataFrame()\n",
    "        df_result[\"y_pred\"] = y_pred\n",
    "        df_result[\"y_prob\"] = frames_probability\n",
    "        df_result[\"y_true\"] = Y_test_fold.tolist()\n",
    "        print(f'Folds {fold_index + 1}')\n",
    "        data = process_folds([df_result])\n",
    "        build_report(pd.DataFrame(data.mean()).T, data.applymap(lambda x:0), \"BiLSTM\")\n",
    "\n",
    "        datasets.append(df_result.copy())\n",
    "    \n",
    "    if writer:\n",
    "        writer.close()\n",
    "    \n",
    "    print(\"Finals results\")\n",
    "    data = process_folds(datasets)\n",
    "    build_report(pd.DataFrame(data.mean()).T, pd.DataFrame(data.std()).T, \"BiLSTM\")\n",
    "    \n",
    "    with open(f\"Results/cross_validation_{name}.pickle\", \"wb\") as file:\n",
    "        pickle.dump(datasets, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 5: Loss: 0.2877 Accuracy: 83.63%Folds 1\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.77(±0.00)       0.76(±0.00)       0.74(±0.00)\n",
      "               Macro       0.72(±0.00)       0.68(±0.00)       0.67(±0.00)       0.63(±0.00)       0.78(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 0.3467 Accuracy: 84.15%Folds 2\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.77(±0.00)       0.77(±0.00)       0.76(±0.00)\n",
      "               Macro       0.71(±0.00)       0.70(±0.00)       0.69(±0.00)       0.61(±0.00)       0.79(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 0.3823 Accuracy: 84.68%Folds 3\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.76(±0.00)       0.77(±0.00)       0.75(±0.00)\n",
      "               Macro       0.70(±0.00)       0.67(±0.00)       0.68(±0.00)       0.61(±0.00)       0.78(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 0.2879 Accuracy: 82.37%Folds 4\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.76(±0.00)       0.76(±0.00)       0.76(±0.00)\n",
      "               Macro       0.71(±0.00)       0.68(±0.00)       0.68(±0.00)       0.61(±0.00)       0.79(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 0.3826 Accuracy: 79.94%Folds 5\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.74(±0.00)       0.76(±0.00)       0.74(±0.00)\n",
      "               Macro       0.68(±0.00)       0.64(±0.00)       0.64(±0.00)       0.55(±0.00)       0.73(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 0.3307 Accuracy: 84.34%Folds 6\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.75(±0.00)       0.73(±0.00)       0.73(±0.00)\n",
      "               Macro       0.69(±0.00)       0.68(±0.00)       0.67(±0.00)       0.59(±0.00)       0.76(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 0.3361 Accuracy: 81.19%Folds 7\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.76(±0.00)       0.76(±0.00)       0.75(±0.00)\n",
      "               Macro       0.72(±0.00)       0.66(±0.00)       0.68(±0.00)       0.59(±0.00)       0.78(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 0.2688 Accuracy: 85.50%Folds 8\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.78(±0.00)       0.78(±0.00)       0.77(±0.00)\n",
      "               Macro       0.72(±0.00)       0.70(±0.00)       0.70(±0.00)       0.63(±0.00)       0.80(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 0.3258 Accuracy: 86.11%Folds 9\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.75(±0.00)       0.75(±0.00)       0.75(±0.00)\n",
      "               Macro       0.69(±0.00)       0.69(±0.00)       0.69(±0.00)       0.61(±0.00)       0.77(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 0.3333 Accuracy: 86.36%Folds 10\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.76(±0.00)       0.77(±0.00)       0.76(±0.00)\n",
      "               Macro       0.73(±0.00)       0.67(±0.00)       0.69(±0.00)       0.59(±0.00)       0.77(±0.00)\n",
      "        \n",
      "Finals results\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.76(±0.02)       0.76(±0.02)       0.75(±0.02)\n",
      "               Macro       0.71(±0.03)       0.68(±0.04)       0.68(±0.04)       0.60(±0.05)       0.77(±0.03)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "args = {'embedding_size': 200, 'hidden_size': 200, 'hidden_dim_lstm': 300,\n",
    "        'n_epochs': 6, 'process_output': 'max', \"batch_size\": 32,\n",
    "        \"loss_function\": \"cross-entropy\"}\n",
    "    \n",
    "cross_validation(args, f'Bi-LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 5: Loss: 0.8863 Accuracy: 79.71%%Folds 1\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.76(±0.00)       0.70(±0.00)       0.71(±0.00)\n",
      "               Macro       0.68(±0.00)       0.71(±0.00)       0.67(±0.00)       0.63(±0.00)       0.78(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 6.6499 Accuracy: 76.02%%Folds 2\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.77(±0.00)       0.70(±0.00)       0.69(±0.00)\n",
      "               Macro       0.69(±0.00)       0.70(±0.00)       0.65(±0.00)       0.59(±0.00)       0.78(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 7.0524 Accuracy: 80.07%%Folds 3\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.79(±0.00)       0.76(±0.00)       0.76(±0.00)\n",
      "               Macro       0.72(±0.00)       0.73(±0.00)       0.71(±0.00)       0.65(±0.00)       0.80(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 6.9781 Accuracy: 75.42%%Folds 4\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.79(±0.00)       0.68(±0.00)       0.66(±0.00)\n",
      "               Macro       0.70(±0.00)       0.70(±0.00)       0.62(±0.00)       0.61(±0.00)       0.78(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 5.0810 Accuracy: 77.69%%Folds 5\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.77(±0.00)       0.71(±0.00)       0.71(±0.00)\n",
      "               Macro       0.68(±0.00)       0.72(±0.00)       0.67(±0.00)       0.62(±0.00)       0.79(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 6.5716 Accuracy: 80.91%%Folds 6\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.77(±0.00)       0.71(±0.00)       0.71(±0.00)\n",
      "               Macro       0.70(±0.00)       0.72(±0.00)       0.68(±0.00)       0.65(±0.00)       0.80(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 5.3720 Accuracy: 79.90%%Folds 7\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.76(±0.00)       0.72(±0.00)       0.73(±0.00)\n",
      "               Macro       0.68(±0.00)       0.71(±0.00)       0.68(±0.00)       0.64(±0.00)       0.78(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 6.8337 Accuracy: 80.02%%Folds 8\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.78(±0.00)       0.72(±0.00)       0.73(±0.00)\n",
      "               Macro       0.68(±0.00)       0.73(±0.00)       0.68(±0.00)       0.64(±0.00)       0.80(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 6.1825 Accuracy: 77.29%%Folds 9\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.75(±0.00)       0.70(±0.00)       0.70(±0.00)\n",
      "               Macro       0.67(±0.00)       0.69(±0.00)       0.65(±0.00)       0.61(±0.00)       0.76(±0.00)\n",
      "        \n",
      "Epoca 5: Loss: 5.2472 Accuracy: 78.33%%Folds 10\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.76(±0.00)       0.70(±0.00)       0.71(±0.00)\n",
      "               Macro       0.67(±0.00)       0.71(±0.00)       0.66(±0.00)       0.59(±0.00)       0.77(±0.00)\n",
      "        \n",
      "Finals results\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.77(±0.02)       0.71(±0.04)       0.71(±0.05)\n",
      "               Macro       0.69(±0.03)       0.71(±0.03)       0.67(±0.05)       0.62(±0.04)       0.78(±0.02)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "args = {'embedding_size': 200, 'hidden_size': 200, 'hidden_dim_lstm': 300,\n",
    "        'n_epochs': 6, 'process_output': 'max', \"batch_size\": 32,\n",
    "        \"loss_function\": \"AsymetricLoss\"}\n",
    "    \n",
    "cross_validation(args, f'Bi-LSTM_AsymetricLoss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
