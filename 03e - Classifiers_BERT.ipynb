{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from auxiliar_functions import process_folds, build_report, load_dataset, predict_deep, load_embedding\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import torch \n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from AsymmetricLoss import AsymmetricLossOptimized\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertConfig, BertTokenizer, BertForPreTraining\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "import gc\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>preprocess_text</th>\n",
       "      <th>encoded</th>\n",
       "      <th>frames</th>\n",
       "      <th>conflicto</th>\n",
       "      <th>economico</th>\n",
       "      <th>humanidad</th>\n",
       "      <th>moral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>japón registró un nuevo déficit comercial réco...</td>\n",
       "      <td>[8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UDI acusa \"mala memoria\" de la Nueva Mayoría f...</td>\n",
       "      <td>udi acusa mala memoria de la nueva mayoría fre...</td>\n",
       "      <td>[9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...</td>\n",
       "      <td>[1, 0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La misteriosa oferta por Esteban Paredes que i...</td>\n",
       "      <td>la misteriosa oferta por esteban paredes que [...</td>\n",
       "      <td>[9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La familia maratón que causó revuelo en Holand...</td>\n",
       "      <td>la familia maratón que causó revuelo en holand...</td>\n",
       "      <td>[9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crean sitio web que recopila mangas descontin...</td>\n",
       "      <td>crean sitio web que [UNK] [UNK] [UNK] para [UN...</td>\n",
       "      <td>[2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  Japón registró un nuevo déficit comercial réco...   \n",
       "1  UDI acusa \"mala memoria\" de la Nueva Mayoría f...   \n",
       "2  La misteriosa oferta por Esteban Paredes que i...   \n",
       "3  La familia maratón que causó revuelo en Holand...   \n",
       "4   Crean sitio web que recopila mangas descontin...   \n",
       "\n",
       "                                     preprocess_text  \\\n",
       "0  japón registró un nuevo déficit comercial réco...   \n",
       "1  udi acusa mala memoria de la nueva mayoría fre...   \n",
       "2  la misteriosa oferta por esteban paredes que [...   \n",
       "3  la familia maratón que causó revuelo en holand...   \n",
       "4  crean sitio web que [UNK] [UNK] [UNK] para [UN...   \n",
       "\n",
       "                                             encoded        frames conflicto  \\\n",
       "0  [8759, 8914, 9989, 9898, 6584, 8773, 8428, 999...  [0, 1, 0, 0]         0   \n",
       "1  [9610, 8486, 8448, 7205, 10001, 9999, 9927, 97...  [1, 0, 0, 1]         1   \n",
       "2  [9999, 1121, 8346, 9990, 8487, 8596, 9996, 1, ...  [1, 0, 0, 0]         1   \n",
       "3  [9999, 9668, 5417, 9996, 7388, 2016, 9997, 887...  [0, 0, 1, 0]         0   \n",
       "4  [2420, 9319, 9360, 9996, 1, 1, 1, 9985, 1, 998...  [0, 1, 0, 0]         0   \n",
       "\n",
       "  economico humanidad moral  \n",
       "0         1         0     0  \n",
       "1         0         0     1  \n",
       "2         0         0     0  \n",
       "3         0         1     0  \n",
       "4         1         0     0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset(\"preprocess_dataset.npy\")\n",
    "Y_true_ = np.array([np.array(x) for x in df.frames])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDER = ['conflicto', 'economico', 'humanidad', 'moral']\n",
    "\n",
    "        \n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "\n",
    "\n",
    "def convert_example_to_feature(example_row):\n",
    "    text, max_seq_length, tokenizer, cls_token, sep_token = example_row\n",
    "    tokens_a = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Account for [CLS] and [SEP] with \"- 2\"\n",
    "    special_tokens_count = 2\n",
    "    tokens_a = tokens_a[:(max_seq_length - special_tokens_count)]\n",
    "    tokens = tokens_a + [sep_token]\n",
    "    segment_ids = [0] * len(tokens)\n",
    "\n",
    "    tokens = [cls_token] + tokens\n",
    "    segment_ids = [0] + segment_ids\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    \n",
    "    input_ids = input_ids + ([0] * padding_length)\n",
    "    input_mask = input_mask + ([0] * padding_length)\n",
    "    segment_ids = segment_ids + ([0] * padding_length)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_examples_to_features(texts, max_seq_length, tokenizer, cls_token, sep_token):\n",
    "    examples = [(text, max_seq_length, tokenizer, cls_token, sep_token) for text in texts]\n",
    "    return [convert_example_to_feature(example) for example in examples]\n",
    "\n",
    "\n",
    "def calculate_pres_recall(preds, Y):\n",
    "    pres_class = [0] * Y.shape[1]\n",
    "    recall_class = [0] * Y.shape[1]\n",
    "    acc_class = [0] * Y.shape[1]\n",
    "\n",
    "    all_y_pred = []\n",
    "    all_y_true = []\n",
    "    for i in range(Y.shape[1]):\n",
    "        y_pred = [int(pred[i]) for pred in preds]\n",
    "        y_true = [int(target[i]) for target in Y]\n",
    "\n",
    "        all_y_pred.extend(y_pred)\n",
    "        all_y_true.extend(y_true)\n",
    "\n",
    "        pres_class[i] = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall_class[i] = recall_score(y_true, y_pred, zero_division=0)\n",
    "        acc_class[i] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    mean_pres = precision_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    mean_recall = recall_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    mean_acc = accuracy_score(all_y_true, all_y_pred)\n",
    "\n",
    "    return mean_pres, mean_recall, mean_acc, pres_class, recall_class, acc_class \n",
    "\n",
    "\n",
    "def save_data(writer, all_logits, Y, total_loss, loss_class,\n",
    "              total_muestras, fold_index, epoch, step):\n",
    "\n",
    "    if writer is None:\n",
    "        return\n",
    "\n",
    "    loss = (total_loss/total_muestras)\n",
    "    pres, recall, acc, pres_class, recall_class, acc_class = calculate_pres_recall(all_logits, Y)\n",
    "\n",
    "    writer.add_scalar(f'Fold_{fold_index}/loss_{step}', loss, epoch)\n",
    "    writer.add_scalar(f'Fold_{fold_index}/recall_{step}', recall, epoch)\n",
    "    writer.add_scalar(f'Fold_{fold_index}/presicion_{step}', pres, epoch)\n",
    "    writer.add_scalar(f'Fold_{fold_index}/acc_{step}', acc, epoch)\n",
    "\n",
    "    for i in range(len(loss_class)):\n",
    "        loss_class_train = loss_class[i]/(total_muestras/Y.shape[1])\n",
    "        writer.add_scalar(f'Fold_{fold_index}/loss_class_{ORDER[i]}_{step}', loss_class_train, epoch)\n",
    "        writer.add_scalar(f'Fold_{fold_index}/presicion_{ORDER[i]}_{step}', pres_class[i], epoch)\n",
    "        writer.add_scalar(f'Fold_{fold_index}/recall_{ORDER[i]}_{step}', recall_class[i], epoch)\n",
    "        writer.add_scalar(f'Fold_{fold_index}/acc_{ORDER[i]}_{step}', acc_class[i], epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetoEmbedding(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, batch_size=8, take_mean=True,\n",
    "                 loss_function=\"cross-entropy\", n_epochs=30):\n",
    "        super().__init__()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('vocab.txt', keep_accents=True)\n",
    "        config = BertConfig.from_json_file('bert_config.json')\n",
    "        config.output_hidden_states = True\n",
    "        self.bert = BertForPreTraining.from_pretrained('pytorch_model.bin', config=config)\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            if name.startswith(\"bert.encoder.layer.11\"):\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        self.take_mean = take_mean\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.loss_object = AsymmetricLossOptimized()\n",
    "        if loss_function == \"cross-entropy\":\n",
    "            self.loss_object = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(768, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.embedding_dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, 4) # Cambiar \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def get_embedding(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        # outputs[-1] = salidas de las capas ocultas (hidden_iniciales, capa_1, ..., capa_12)\n",
    "        outputs = outputs[-1][-1] # Tomar de las hidden_state, la última capa\n",
    "        if self.take_mean:\n",
    "            output = torch.mean(outputs, 1)\n",
    "        else:\n",
    "            output = outputs[:, 0, :] # Tomar el primer vector (CLS)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.get_embedding(input_ids, attention_mask, token_type_ids)\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "  \n",
    "\n",
    "    def get_proba(self, input_ids, attention_mask, token_type_ids):\n",
    "        return self.sigmoid(self(input_ids, attention_mask, token_type_ids))\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y, X_val=None, Y_val=None, writer=None, fold_index=None):\n",
    "        model = self.cuda()\n",
    "        self.bert.cuda()\n",
    "        tokenizer = self.tokenizer\n",
    "        \n",
    "        features = convert_examples_to_features(X, 512, tokenizer, tokenizer.cls_token,tokenizer.sep_token)\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "\n",
    "        train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, torch.Tensor(Y))\n",
    "        train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        dataloaders = {\"train\": train_dl}\n",
    "        \n",
    "        if X_val is not None:\n",
    "            features = convert_examples_to_features(X_val, 512, tokenizer, tokenizer.cls_token,tokenizer.sep_token)\n",
    "            all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "            all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "            all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "            val_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, torch.Tensor(Y_val))\n",
    "            dataloaders[\"val\"] = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        \n",
    "        optimizer = AdamW(model.parameters(), lr=0.001)  \n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            for step in dataloaders:\n",
    "                if step == \"train\":\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    \n",
    "                total_muestras = 0.0\n",
    "                total_correctas = 0.0\n",
    "                total_loss = 0\n",
    "                loss_class = [0, 0, 0, 0]\n",
    "                all_logits = None\n",
    "                all_target = None\n",
    "\n",
    "                for batch in dataloaders[step]:\n",
    "                    optimizer.zero_grad()                    \n",
    "                    \n",
    "                    input_ids, attention_mask, token_type_ids, target = [x.cuda() for x in batch]\n",
    "                    target = target.cuda()                   \n",
    "                    logits = model(input_ids, attention_mask, token_type_ids)          \n",
    "\n",
    "                    loss = self.loss_object(logits, target)\n",
    "                    total_loss += (loss * (target.shape[0] * target.shape[1])).item()\n",
    "\n",
    "                    for i in range(target.shape[1]):\n",
    "                        loss_c = self.loss_object(logits[:, i], target[:, i])\n",
    "                        loss_class[i] += (loss_c * target.shape[0]).item()\n",
    "\n",
    "                    preds = logits >= 0.0\n",
    "                    if all_logits is None:\n",
    "                        all_logits = preds.detach().cpu().numpy()\n",
    "                        all_target = target.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        all_logits = np.append(all_logits, preds.detach().cpu().numpy(), axis=0)\n",
    "                        all_target = np.append(all_target, target.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "                    total_muestras += (target.shape[0]*target.shape[1])       # Sumamos el tamaño del batch\n",
    "                    \n",
    "                    if step == \"train\":\n",
    "                        loss.backward()                             # Backpropagation\n",
    "                        optimizer.step()                            # Actualizamos parámetros\n",
    "                        correctas = (preds == target).sum().item()  # Acumulamos las correctas durante la época\n",
    "                        total_correctas += correctas               \n",
    "                        accuracy = total_correctas/total_muestras \n",
    "                        \n",
    "                        print(\"\\rEpoca {}: Loss: {:.4f} Accuracy: {:.2f}%\".format(epoch, loss, 100*accuracy),\n",
    "                              end=\"\")\n",
    "\n",
    "                save_data(writer, all_logits, all_target, total_loss, loss_class,\n",
    "                          total_muestras, fold_index, epoch, step)\n",
    "\n",
    "                    \n",
    "    def predict_proba(self, X):\n",
    "        model = self.cuda()\n",
    "        self.bert.cuda()\n",
    "        model.eval()\n",
    "        self.bert.eval()\n",
    "        \n",
    "        tokenizer = self.tokenizer\n",
    "        features = convert_examples_to_features(X, 512, tokenizer, tokenizer.cls_token,tokenizer.sep_token)\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "\n",
    "        train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "        train_dl = DataLoader(train_dataset, batch_size=1)\n",
    "        \n",
    "        all_probs = None\n",
    "        for batch in train_dl:\n",
    "            input_ids, attention_mask, token_type_ids = [x.cuda() for x in batch]\n",
    "            probs = model.get_proba(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            if all_probs is None:\n",
    "                all_probs = probs.detach().cpu().numpy()\n",
    "            else:\n",
    "                all_probs = np.append(all_probs, probs.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        return all_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(args, name):\n",
    "    writer = SummaryWriter(f'runs/{name}')\n",
    "    \n",
    "    embedding_train = df.preprocess_text.values\n",
    "    Y_true = Y_true_\n",
    "    \n",
    "    np.random.seed(4444)\n",
    "    torch.manual_seed(4444)\n",
    "\n",
    "    datasets = []\n",
    "    k_fold = IterativeStratification(n_splits=10, order=1)\n",
    "\n",
    "    for fold_index, (train, test) in enumerate(k_fold.split(embedding_train, Y_true)):\n",
    "\n",
    "        Y_train_fold, Y_test_fold = Y_true[train], Y_true[test]\n",
    "        X_train_fold, X_test_fold = embedding_train[train], embedding_train[test]\n",
    "\n",
    "        model = BetoEmbedding(**args)\n",
    "        model.fit(X_train_fold, Y_train_fold, X_test_fold, Y_test_fold, writer, fold_index)\n",
    "        torch.save(model.state_dict(), f'Models/Fold_{fold_index+1}_{name}.model')\n",
    "        \n",
    "        frames_probability = model.predict_proba(X_test_fold).tolist()\n",
    "        y_pred = [[int(pred >= 0.5) for pred in frames] for frames in frames_probability]\n",
    "\n",
    "        df_result = pd.DataFrame()\n",
    "        df_result[\"y_pred\"] = y_pred\n",
    "        df_result[\"y_prob\"] = frames_probability\n",
    "        df_result[\"y_true\"] = Y_test_fold.tolist()\n",
    "        print(f'Folds {fold_index + 1}')\n",
    "        data = process_folds([df_result])\n",
    "        build_report(pd.DataFrame(data.mean()).T, data.applymap(lambda x:0), \"Beto_Finetunning\")\n",
    "\n",
    "        datasets.append(df_result.copy())\n",
    "        del model.bert\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "    print(\"Finals results\")\n",
    "    data = process_folds(datasets)\n",
    "    build_report(pd.DataFrame(data.mean()).T, pd.DataFrame(data.std()).T, \"Beto_Finetunning\")\n",
    "\n",
    "    with open(f\"Results/cross_validation_{name}.pickle\", \"wb\") as file:\n",
    "        pickle.dump(datasets, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 3: Loss: 0.3484 Accuracy: 81.21%Folds 1\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.79(±0.00)       0.79(±0.00)       0.78(±0.00)\n",
      "               Macro       0.75(±0.00)       0.71(±0.00)       0.72(±0.00)       0.66(±0.00)       0.82(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 3: Loss: 0.4154 Accuracy: 82.00%Folds 2\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.80(±0.00)       0.79(±0.00)       0.79(±0.00)\n",
      "               Macro       0.76(±0.00)       0.73(±0.00)       0.74(±0.00)       0.69(±0.00)       0.84(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 3: Loss: 0.4900 Accuracy: 81.77%Folds 3\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.80(±0.00)       0.80(±0.00)       0.80(±0.00)\n",
      "               Macro       0.77(±0.00)       0.73(±0.00)       0.74(±0.00)       0.70(±0.00)       0.84(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 3: Loss: 0.3573 Accuracy: 81.99%Folds 4\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.77(±0.00)       0.77(±0.00)       0.76(±0.00)\n",
      "               Macro       0.73(±0.00)       0.68(±0.00)       0.68(±0.00)       0.65(±0.00)       0.82(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 3: Loss: 0.3444 Accuracy: 81.91%Folds 5\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.79(±0.00)       0.79(±0.00)       0.79(±0.00)\n",
      "               Macro       0.74(±0.00)       0.74(±0.00)       0.74(±0.00)       0.65(±0.00)       0.82(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 3: Loss: 0.3604 Accuracy: 81.68%Folds 6\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.79(±0.00)       0.79(±0.00)       0.79(±0.00)\n",
      "               Macro       0.75(±0.00)       0.72(±0.00)       0.73(±0.00)       0.68(±0.00)       0.83(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 3: Loss: 0.4315 Accuracy: 81.74%Folds 7\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.78(±0.00)       0.78(±0.00)       0.78(±0.00)\n",
      "               Macro       0.74(±0.00)       0.73(±0.00)       0.73(±0.00)       0.68(±0.00)       0.83(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 3: Loss: 0.3574 Accuracy: 82.01%Folds 8\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.80(±0.00)       0.81(±0.00)       0.80(±0.00)\n",
      "               Macro       0.76(±0.00)       0.73(±0.00)       0.75(±0.00)       0.72(±0.00)       0.85(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 3: Loss: 0.3445 Accuracy: 82.00%Folds 9\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.80(±0.00)       0.80(±0.00)       0.80(±0.00)\n",
      "               Macro       0.75(±0.00)       0.74(±0.00)       0.74(±0.00)       0.67(±0.00)       0.82(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 3: Loss: 0.4014 Accuracy: 82.14%Folds 10\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.79(±0.00)       0.79(±0.00)       0.79(±0.00)\n",
      "               Macro       0.76(±0.00)       0.73(±0.00)       0.73(±0.00)       0.69(±0.00)       0.82(±0.00)\n",
      "        \n",
      "Finals results\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.79(±0.02)       0.79(±0.02)       0.79(±0.02)\n",
      "               Macro       0.75(±0.03)       0.72(±0.04)       0.73(±0.04)       0.68(±0.05)       0.83(±0.02)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "args = {'hidden_size': 150, \"batch_size\": 32, 'take_mean': True, 'n_epochs': 4, 'loss_function': \"cross-entropy\"}\n",
    "    \n",
    "cross_validation(args, 'Beto-finetunning_cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 9: Loss: 1.9305 Accuracy: 77.01%%Folds 1\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.78(±0.00)       0.70(±0.00)       0.69(±0.00)\n",
      "               Macro       0.69(±0.00)       0.71(±0.00)       0.65(±0.00)       0.65(±0.00)       0.79(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 9: Loss: 8.6965 Accuracy: 77.01%%Folds 2\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.81(±0.00)       0.71(±0.00)       0.69(±0.00)\n",
      "               Macro       0.73(±0.00)       0.73(±0.00)       0.66(±0.00)       0.69(±0.00)       0.83(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 9: Loss: 9.3006 Accuracy: 76.57%%Folds 3\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.82(±0.00)       0.72(±0.00)       0.71(±0.00)\n",
      "               Macro       0.73(±0.00)       0.73(±0.00)       0.67(±0.00)       0.67(±0.00)       0.82(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 9: Loss: 7.6648 Accuracy: 76.91%%Folds 4\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.80(±0.00)       0.73(±0.00)       0.72(±0.00)\n",
      "               Macro       0.71(±0.00)       0.74(±0.00)       0.68(±0.00)       0.62(±0.00)       0.81(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 9: Loss: 8.2436 Accuracy: 77.31%%Folds 5\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.78(±0.00)       0.70(±0.00)       0.68(±0.00)\n",
      "               Macro       0.69(±0.00)       0.72(±0.00)       0.64(±0.00)       0.68(±0.00)       0.82(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 9: Loss: 7.5762 Accuracy: 76.47%%Folds 6\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.81(±0.00)       0.74(±0.00)       0.73(±0.00)\n",
      "               Macro       0.74(±0.00)       0.75(±0.00)       0.70(±0.00)       0.67(±0.00)       0.82(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 9: Loss: 6.5144 Accuracy: 75.33%%Folds 7\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.80(±0.00)       0.72(±0.00)       0.70(±0.00)\n",
      "               Macro       0.72(±0.00)       0.73(±0.00)       0.66(±0.00)       0.68(±0.00)       0.81(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 9: Loss: 4.2287 Accuracy: 74.41%%Folds 8\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.77(±0.00)       0.68(±0.00)       0.65(±0.00)\n",
      "               Macro       0.68(±0.00)       0.71(±0.00)       0.62(±0.00)       0.71(±0.00)       0.83(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 9: Loss: 10.2238 Accuracy: 74.47%Folds 9\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.80(±0.00)       0.72(±0.00)       0.71(±0.00)\n",
      "               Macro       0.72(±0.00)       0.73(±0.00)       0.68(±0.00)       0.67(±0.00)       0.82(±0.00)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at pytorch_model.bin and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 9: Loss: 6.5152 Accuracy: 75.55%%Folds 10\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.81(±0.00)       0.72(±0.00)       0.71(±0.00)\n",
      "               Macro       0.73(±0.00)       0.74(±0.00)       0.68(±0.00)       0.67(±0.00)       0.82(±0.00)\n",
      "        \n",
      "Finals results\n",
      "\n",
      "        Mean                \n",
      "                            precision           recall         f1-score              AUC          ROC AUC\n",
      "        \n",
      "               Micro       0.80(±0.03)       0.71(±0.04)       0.70(±0.04)\n",
      "               Macro       0.71(±0.04)       0.73(±0.03)       0.66(±0.04)       0.67(±0.05)       0.82(±0.02)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "args = {'hidden_size': 150, \"batch_size\": 32, 'take_mean': True, 'n_epochs': 10, 'loss_function': \"asymetric\"}\n",
    "    \n",
    "cross_validation(args, 'Beto-finetunning_asymetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
